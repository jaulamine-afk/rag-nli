# RAG with NLI and Sub-Claim Decomposition

[FranÃ§ais](README.fr.md)

This project explores how Natural Language Inference (NLI) and claim decomposition can be integrated into a Retrieval-Augmented Generation (RAG) pipeline to reduce retrieval noise and improve answer grounding.


## Motivation

Standard RAG systems often retrieve passages that are:

- loosely related to the question,
- partially contradictory,
- or irrelevant but semantically similar.

This noise can confuse the generator and degrade answer quality.

This project proposes:

- using NLI-based entailment filtering to keep only passages that logically support a claim,
- and an extension based on sub-claim decomposition for comparative or multi-entity questions.

## Approach Overview

Three pipelines are implemented and compared:

### RAG Baseline

- Dense retrieval (FAISS)
- Prompt-based generation

### RAG + NLI

- Retrieved passages are filtered using an NLI model
- Only passages that entail the claim are kept

### RAG + NLI + Sub-Claims

- Complex claims are decomposed into simpler sub-claims
- Each sub-claim is validated independently with NLI
- Passages are kept only if they support at least one sub-claim

This allows finer-grained filtering, especially for comparative or compositional questions.

## Evaluation

Experiments were conducted on HotpotQA (distractor setting).

**Metrics used:**

- Exact Match
- F1
- BERTScore (Precision / Recall / F1)

Results show consistent improvements over the RAG baseline, with:

- reduced irrelevant passages,
- improved answer grounding,
- and clearer gains for composition-heavy questions.

## Analysis Agent (Demonstration)

In addition to quantitative evaluation, the project includes an analysis agent powered by Gemini.

This agent:

- compares answers from RAG vs RAG + NLI Sub-Claims,
- inspects retrieved passages before and after filtering,
- explains why one pipeline produces a more reliable answer.

This component is intended as a pedagogical and interpretability tool, not as part of the core evaluation loop.

## Project Structure

```
rag-nli/
â”‚
â”œâ”€â”€ rag/                 # Retrieval & generation
â”œâ”€â”€ nli/                 # NLI model and filtering logic
â”œâ”€â”€ pipelines/           # RAG / RAG+NLI / RAG+NLI+Subclaim
â”œâ”€â”€ evaluation/          # Metrics and experiments
â”œâ”€â”€ agents/              # Analysis agent
â”œâ”€â”€ api/                 # FastAPI service
â”œâ”€â”€ scripts/             # Experiment runners
â”œâ”€â”€ data/
â””â”€â”€ README.md
```

## Running the Project

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Run experiments

```bash
python scripts/run_experiments.py
```

This will run all pipelines on a subset of HotpotQA and output evaluation metrics.

### 3. Run the API

The project exposes a FastAPI service for question answering.

```bash
python -m uvicorn api.main:app --host 127.0.0.1 --port 8001
```

## API Key Configuration (Gemini)

Some components (analysis agent) use Gemini 2.5 Flash.

You need to generate an API key here:  
ðŸ‘‰ https://aistudio.google.com/app/apikey

Paste your API key in the `code_api.txt` file at the root of the project.

## Limitations

- Sub-claim decomposition is rule-based and heuristic
- Not all claims in HotpotQA are decomposable
- No statistical significance testing (CPU-only setup)
- Focus is on retrieval noise reduction, not full hallucination prevention

These limitations are discussed transparently to emphasize realism and reproducibility.

## Technologies

- Python
- Hugging Face
- FAISS
- FastAPI
- LangChain / LangGraph
- Docker
- AWS
- Gemini (Google GenAI)
